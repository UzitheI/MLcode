#we can calculate the working of our gradient descent by seeing its learning curve 

#learning curve can be calculated by plotting the cost function value against the number of iterations 

# with each consecutive iteration we find that the cost function is decreasing

#if the model is working properly then the curve should show that with every consecutive iteration the cost funciton is decreasing 

#also after a certain number of iterations we might find that the value of cost function is same which means that the cost function value has converged 

#if it is not converging then the model is wrong or the convergence rate has been chosen poorly 

#a learning curve can also be used to test the number of iterations required for the model to converge the value of the cost function 

#in order to know the number of iterations required we can also use the automatic convergence test

#e be epislon and the value be 10^-3 , if the cost function value is less than or equals to epsilon in one iteraton then declare convergence 

# convergence means that we have found the correct value of w and b to reach the correct glbal minimum


